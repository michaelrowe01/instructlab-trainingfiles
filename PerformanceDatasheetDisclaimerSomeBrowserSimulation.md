META:TOPICINFO{author="gcovell" date="1400789627" format="1.1"
reprev="1.1" version="1.1"}
META:TOPICPARENT{name="PerformanceDatasheetsAndSizingGuidelines"}

The information in this document is distributed AS IS. The use of this
information or the implementation of any of these techniques is a
customer responsibility and depends on the customers ability to evaluate
and integrate them into the customers operational environment. While
each item may have been reviewed by IBM for accuracy in a specific
situation, there is no guarantee that the same or similar results will
be obtained elsewhere. Customers attempting to adapt these techniques to
their own environments do so at their own risk. Any pointers in this
publication to external Web sites are provided for convenience only and
do not in any manner serve as an endorsement of these Web sites. Any
performance data contained in this document was determined in a
controlled environment, and therefore, the results that may be obtained
in other operating environments may vary significantly. Users of this
document should verify the applicable data for their specific
environment.

Performance is based on measurements and projections using standard IBM
benchmarks in a controlled environment. The actual throughput or
performance that any user will experience will vary depending upon many
factors, including considerations such as the amount of
multi-programming in the users job stream, the I/O configuration, the
storage configuration, and the workload processed. Therefore, no
assurance can be given that an individual user will achieve results
similar to those stated here.

This testing was done as a way to compare and characterize the
differences in performance between different versions of the product.
The results shown here should thus be looked at as a comparison of the
contrasting performance between different versions, and not as an
absolute benchmark of performance.

### What our tests measure

We use predominantly automated tooling such as Rational Performance
Tester (RPT) to simulate a workload normally generated by client
software such as the Eclipse client or web browsers. All response times
listed are those measured by our automated tooling and not a client.

The diagram below describes at a very high level which aspects of the
entire end-to-end experience (human end-user to server and back again)
that our performance tests simulate. The tests described in this article
simulate a large part of the end-to-end transaction as indicated.
Performance tests include some simulation of browser rendering and
network latency between the simulated browser client and the application
server stack.

META:FILEATTACHMENT{name="schematicperformancetestmap_somebrowsersim.jpg"
attachment="schematicperformancetestmap_somebrowsersim.jpg" attr="h"
comment="" date="1400789157"
path="schematicperformancetestmap_somebrowsersim.jpg" size="53506"
user="gcovell" version="1"}
